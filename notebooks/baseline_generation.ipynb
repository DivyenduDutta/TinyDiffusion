{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "An important thing to keep in mind here is that a stable diffusion model is not a monolithic model but has different parts such as a UNet, VAE, text encoders etc. \n",
    "\n",
    "_Here (at least initially) we will only focus on and benchmark the UNet_ because its the most compute heavy. And this approach is more simpler than trying to export to ONNX, apply PTQ, QAT etc on all the components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "import time\n",
    "import statistics\n",
    "import psutil\n",
    "import pandas as pd \n",
    "from tinydiffusion.utils.logger import LoggerConfig #this works in VS Code because of the .env file\n",
    "from tinydiffusion.utils.constants import PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGGER = LoggerConfig().logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "LOGGER.info(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "Load a lightweight/distilled Stable Diffusion model (LoRA or small variant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = os.path.dirname(os.getcwd())\n",
    "LOGGER.info(f\"Root directory: {ROOT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tinydiffusion.utils.constants import ModelType\n",
    "\n",
    "# Example: \"stabilityai/stable-diffusion-2-base\" is smaller than SD 1.5 full\n",
    "model_cache_dir = os.path.join(ROOT_DIR, \"checkpoints\", \"stablediffusion\")\n",
    "model_id = ModelType.STABLE_DIFFUSION_2_BASE.value  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "Below we load the fp16 variant (as opposed to downloading the fp32 variant and then converting to fp16). [Ref](https://huggingface.co/docs/diffusers/en/using-diffusers/loading#:~:text=There%20are%20two%20important%20arguments%20for%20loading%20variants%3A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pipeline\n",
    "if device == \"cuda\":\n",
    "    pipe = StableDiffusionPipeline.from_pretrained(\n",
    "        model_id,\n",
    "        cache_dir=model_cache_dir,\n",
    "        variant=\"fp16\",        \n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "else:\n",
    "    # for CPU use fp32 if available\n",
    "    pipe = StableDiffusionPipeline.from_pretrained(\n",
    "        model_id,\n",
    "        cache_dir=model_cache_dir,\n",
    "        torch_dtype=torch.float32 \n",
    "    )\n",
    "pipe = pipe.to(device)\n",
    "pipe.unet.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "[StableDiffusionPipeline.enable_attention_slicing()](https://huggingface.co/docs/diffusers/v0.3.0/en/api/pipelines/stable_diffusion#diffusers.StableDiffusionPipeline.enable_attention_slicing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable memory-efficient attention for less VRAM usage\n",
    "pipe.enable_attention_slicing()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "Create inputs of UNet since as noted at the beginning of the notebook, we intend to benchmark just the UNet. So we need to explicitly pass the inputs to the UNet and get just the UNet ouput.\n",
    "\n",
    "See [this](https://medium.com/@onkarmishra/stable-diffusion-explained-1f101284484d) for a quick reference about the stable diffusion architecture.\n",
    "\n",
    "Instead of running text encoder → U-Net denoising loop → VAE decode we:\n",
    "\n",
    "- Generate fake random latents (the \"noisy\" image at some timestep).\n",
    "- Pick a timestep (e.g. 50).\n",
    "- Encode text prompt via `pipe.text_encoder` ie, Stable Diffusion's text encoder (whatever it uses).\n",
    "- Run just the U-Net forward pass.\n",
    "\n",
    "This means we dont actually generate a final image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "height = width = 64\n",
    "latents = torch.randn(\n",
    "    (batch_size, pipe.unet.config.in_channels, height, width),\n",
    "    device=device,\n",
    "    dtype=pipe.unet.dtype\n",
    ")\n",
    "timestep = torch.tensor([10], device=device, dtype=torch.int64)  # arbitrary diffusion step\n",
    "text_embeddings = pipe.text_encoder(\n",
    "    pipe.tokenizer(PROMPT, return_tensors=\"pt\").input_ids.to(device)\n",
    ")[0]\n",
    "\n",
    "LOGGER.info(f\"Text embeddings shape: {text_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "inference_time = []\n",
    "cpu_mem_usage = []\n",
    "gpu_mem_usage = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PROMPT\n",
    "num_samples = 10\n",
    "\n",
    "process = psutil.Process(os.getpid())\n",
    "\n",
    "#GEN_IMG_SAVE_PATH = os.path.join(os.path.dirname(os.getcwd()), \"results\", \"generated_images\")\n",
    "#os.makedirs(GEN_IMG_SAVE_PATH, exist_ok=True)\n",
    "\n",
    "#LOGGER.info(f\"Generating images. Will be saved to: {GEN_IMG_SAVE_PATH}\")\n",
    "\n",
    "results = []\n",
    "\n",
    "for i in range(num_samples):\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        #image = pipe(prompt, guidance_scale=7.5, num_inference_steps=50).images[0] # this is what we would typically do to generate the image\n",
    "        noise_pred = pipe.unet(latents, timestep, text_embeddings).sample\n",
    "    end_time = time.time()\n",
    "    inference_time.append(end_time - start_time)\n",
    "\n",
    "    # Memory usage - START\n",
    "    cpu_mem = process.memory_info().rss / (1024**2)  # MB\n",
    "    cpu_mem_usage.append(cpu_mem)\n",
    "\n",
    "    if device == \"cuda\":\n",
    "        gpu_mem = torch.cuda.memory_allocated(0) / (1024**2)  # MB\n",
    "        gpu_mem_usage.append(gpu_mem)\n",
    "    else:\n",
    "        gpu_mem = 0 \n",
    "    # Memory usage - END\n",
    "\n",
    "    LOGGER.info(f\"UNet Inference time: {(end_time - start_time):.2f}s\")\n",
    "\n",
    "LOGGER.info(f\"\\nAverage inference time: {statistics.mean(inference_time):.2f}s ± {statistics.stdev(inference_time):.2f}s\")\n",
    "LOGGER.info(f\"\\nAverage CPU memory usage: {statistics.mean(cpu_mem_usage):.2f}MB ± {statistics.stdev(cpu_mem_usage):.2f}MB\")\n",
    "if device == \"cuda\":\n",
    "    LOGGER.info(f\"\\nAverage GPU memory usage: {statistics.mean(gpu_mem_usage):.2f}MB ± {statistics.stdev(gpu_mem_usage):.2f}MB\")\n",
    "\n",
    "# store results\n",
    "results.append({\n",
    "    \"desc\": \"stable_diffusion_UNet_GPU\",\n",
    "    \"avg_inference_time\": statistics.mean(inference_time),\n",
    "    \"std_inference_time\": statistics.stdev(inference_time),\n",
    "    \"avg_cpu_mem_usage\": statistics.mean(cpu_mem_usage),\n",
    "    \"std_cpu_mem_usage\": statistics.stdev(cpu_mem_usage),\n",
    "    \"avg_gpu_mem_usage\": statistics.mean(gpu_mem_usage),\n",
    "    \"std_gpu_mem_usage\": statistics.stdev(gpu_mem_usage),\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "The inference time would be really small here because we're running only one denoising step of the UNet as opposed to say 50 denoising steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "Save benchmark details as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "BENCHMARK_SAVE_PATH = os.path.join(os.path.dirname(os.getcwd()), \"results\", \"benchmarks\")\n",
    "os.makedirs(BENCHMARK_SAVE_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)\n",
    "csv_path = os.path.join(BENCHMARK_SAVE_PATH, \"benchmark_results.csv\")\n",
    "df.to_csv(csv_path, index=False)\n",
    "LOGGER.info(f\"Saved benchmark results to {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tiny-diffusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
